{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ee40445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2065c4",
   "metadata": {},
   "source": [
    "# Question 3: Hyperparameter Tuning & Model Optimization\n",
    "## Scenario\n",
    "-After deploying your classification model, stakeholders ask you to improve performance without changing the dataset.\n",
    "\n",
    "-You decide to optimize the model using systematic hyperparameter search techniques.\n",
    "\n",
    "# Dataset\n",
    "-Reuse the dataset from Question 2.\n",
    "\n",
    "# Tasks\n",
    "- 1 Explain the difference between parameters and hyperparameters.\n",
    "- 2 Train a baseline Logistic Regression model.\n",
    "- 3 Apply GridSearchCV to tune hyperparameters (C, penalty, solver).\n",
    "- 4 Apply RandomizedSearchCV and compare results.\n",
    "- 5 Compare performance before and after tuning.\n",
    "- 6 Discuss trade-offs between computational cost and model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d56a0ac",
   "metadata": {},
   "source": [
    "## 1 Explain the difference between parameters and hyperparameters \n",
    "### A Parameters\n",
    " - Learned from data during training.\n",
    " - Model “internal weights”.\n",
    " - Example in Logistic Regression: coefficients (β) and intercept (β0).\n",
    "### B Hyperparameters\n",
    " - Set before training (you choose them).\n",
    " - Control how the model learns / regularizes.\n",
    " - Examples in Logistic Regression:\n",
    "   - C (regularization strength)\n",
    "   -  penalty (l1, l2, elasticnet, none)\n",
    "   - solver (liblinear, lbfgs, saga, etc.)\n",
    "   - max_iter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c2798d",
   "metadata": {},
   "source": [
    "## Encoding and Feature Scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45470f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"WA_Fn-UseC_-HR-Employee-Attrition.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72b2a923",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Attrition',axis=1)\n",
    "y = data['Attrition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c47e5573",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.map({'Yes': 1, 'No': 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0337a862",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = X.select_dtypes(include='object').columns\n",
    "X_encoded = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e74f4d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Age', 'DailyRate', 'DistanceFromHome', 'Education', 'EmployeeCount',\n",
      "       'EmployeeNumber', 'EnvironmentSatisfaction', 'HourlyRate',\n",
      "       'JobInvolvement', 'JobLevel', 'JobSatisfaction', 'MonthlyIncome',\n",
      "       'MonthlyRate', 'NumCompaniesWorked', 'PercentSalaryHike',\n",
      "       'PerformanceRating', 'RelationshipSatisfaction', 'StandardHours',\n",
      "       'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear',\n",
      "       'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole',\n",
      "       'YearsSinceLastPromotion', 'YearsWithCurrManager'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "num_cols = X_encoded.select_dtypes(include=['int64', 'float64']).columns\n",
    "print(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cdf57efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70a01ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "X_test[num_cols] = scaler.transform(X_test[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc947a54",
   "metadata": {},
   "source": [
    "## 2 Train a baseline Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "111e2d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE Confusion Matrix:\n",
      " [[238   9]\n",
      " [ 31  16]]\n",
      "\n",
      "BASELINE Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92       247\n",
      "           1       0.64      0.34      0.44        47\n",
      "\n",
      "    accuracy                           0.86       294\n",
      "   macro avg       0.76      0.65      0.68       294\n",
      "weighted avg       0.85      0.86      0.85       294\n",
      "\n",
      "BASELINE ROC-AUC: 0.8094581789990525\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Baseline model\n",
    "baseline = LogisticRegression(max_iter=500, random_state=42)\n",
    "\n",
    "baseline.fit(X_train, y_train)\n",
    "\n",
    "y_pred_base = baseline.predict(X_test)\n",
    "y_proba_base = baseline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"BASELINE Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_base))\n",
    "print(\"\\nBASELINE Classification Report:\\n\", classification_report(y_test, y_pred_base))\n",
    "print(\"BASELINE ROC-AUC:\", roc_auc_score(y_test, y_proba_base))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f0b1ed",
   "metadata": {},
   "source": [
    "## 3 Apply GridSearchCV to tune hyperparameters (C, penalty, solver)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5a3f763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best GridSearch Params: {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Best GridSearch CV Score (F1): 0.5729401154401155\n",
      "\n",
      "GRID Confusion Matrix:\n",
      " [[239   8]\n",
      " [ 31  16]]\n",
      "\n",
      "GRID Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.92       247\n",
      "           1       0.67      0.34      0.45        47\n",
      "\n",
      "    accuracy                           0.87       294\n",
      "   macro avg       0.78      0.65      0.69       294\n",
      "weighted avg       0.85      0.87      0.85       294\n",
      "\n",
      "GRID ROC-AUC: 0.801016452752175\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "param_grid = [\n",
    "    {\"solver\": [\"liblinear\"], \"penalty\": [\"l1\", \"l2\"], \"C\": [0.01, 0.1, 1, 10, 100]},\n",
    "    {\"solver\": [\"lbfgs\"], \"penalty\": [\"l2\"], \"C\": [0.01, 0.1, 1, 10, 100]},\n",
    "    {\"solver\": [\"saga\"], \"penalty\": [\"l1\", \"l2\"], \"C\": [0.01, 0.1, 1, 10, 100]},\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    LogisticRegression(max_iter=2000, random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"f1\",        \n",
    "    cv=cv,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best GridSearch Params:\", grid.best_params_)\n",
    "print(\"Best GridSearch CV Score (F1):\", grid.best_score_)\n",
    "\n",
    "best_grid = grid.best_estimator_\n",
    "y_pred_grid = best_grid.predict(X_test)\n",
    "y_proba_grid = best_grid.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nGRID Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_grid))\n",
    "print(\"\\nGRID Classification Report:\\n\", classification_report(y_test, y_pred_grid))\n",
    "print(\"GRID ROC-AUC:\", roc_auc_score(y_test, y_proba_grid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4884d37",
   "metadata": {},
   "source": [
    "## 4 Apply RandomizedSearchCV and compare results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80f084b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearch error (likely invalid combo). Fixing by using error_score=np.nan...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nepal\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:516: FitFailedWarning: \n",
      "40 fits failed out of a total of 150.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "40 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\nepal\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\nepal\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\nepal\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1218, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\nepal\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\nepal\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [0.43796282 0.556114   0.         0.06978111 0.56120557 0.\n",
      " 0.29415571 0.56120557 0.                nan 0.                nan\n",
      " 0.         0.                nan 0.556114          nan 0.\n",
      " 0.55754257        nan 0.52265044 0.53788154 0.5556769  0.556114\n",
      " 0.52265044        nan        nan 0.43516583        nan 0.55026984]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Randomized Params: {'C': np.float64(4.0428727350273315), 'penalty': 'l2', 'solver': 'saga'}\n",
      "Best Randomized CV Score (F1): 0.5612055723290428\n",
      "\n",
      "RANDOM Confusion Matrix:\n",
      " [[237  10]\n",
      " [ 31  16]]\n",
      "\n",
      "RANDOM Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92       247\n",
      "           1       0.62      0.34      0.44        47\n",
      "\n",
      "    accuracy                           0.86       294\n",
      "   macro avg       0.75      0.65      0.68       294\n",
      "weighted avg       0.84      0.86      0.84       294\n",
      "\n",
      "RANDOM ROC-AUC: 0.8091997588078215\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "param_dist = {\n",
    "    \"solver\": [\"liblinear\", \"lbfgs\", \"saga\"],\n",
    "    \"penalty\": [\"l1\", \"l2\"],  # randomized will try invalid combos sometimes; we'll handle by error_score\n",
    "    \"C\": loguniform(1e-3, 1e3),\n",
    "}\n",
    "\n",
    "rand = RandomizedSearchCV(\n",
    "    LogisticRegression(max_iter=2000, random_state=42),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,\n",
    "    scoring=\"f1\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    error_score=\"raise\"  # if this raises due to invalid combo, set to np.nan\n",
    ")\n",
    "\n",
    "# If you get errors due to invalid (solver, penalty) combos:\n",
    "# change error_score=np.nan and rerun\n",
    "try:\n",
    "    rand.fit(X_train, y_train)\n",
    "except Exception as e:\n",
    "    print(\"RandomizedSearch error (likely invalid combo). Fixing by using error_score=np.nan...\")\n",
    "    import numpy as np\n",
    "    rand = RandomizedSearchCV(\n",
    "        LogisticRegression(max_iter=2000, random_state=42),\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=30,\n",
    "        scoring=\"f1\",\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        error_score=np.nan\n",
    "    )\n",
    "    rand.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Randomized Params:\", rand.best_params_)\n",
    "print(\"Best Randomized CV Score (F1):\", rand.best_score_)\n",
    "\n",
    "best_rand = rand.best_estimator_\n",
    "y_pred_rand = best_rand.predict(X_test)\n",
    "y_proba_rand = best_rand.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nRANDOM Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rand))\n",
    "print(\"\\nRANDOM Classification Report:\\n\", classification_report(y_test, y_pred_rand))\n",
    "print(\"RANDOM ROC-AUC:\", roc_auc_score(y_test, y_proba_rand))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dff9978",
   "metadata": {},
   "source": [
    "## 5 Compare performance before and after tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aad09365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: {'Accuracy': 0.8639455782312925, 'Precision': 0.64, 'Recall': 0.3404255319148936, 'F1': 0.4444444444444444, 'ROC-AUC': 0.8094581789990525}\n",
      "GridSearch: {'Accuracy': 0.8673469387755102, 'Precision': 0.6666666666666666, 'Recall': 0.3404255319148936, 'F1': 0.4507042253521127, 'ROC-AUC': 0.801016452752175}\n",
      "RandomSearch: {'Accuracy': 0.8605442176870748, 'Precision': 0.6153846153846154, 'Recall': 0.3404255319148936, 'F1': 0.4383561643835616, 'ROC-AUC': 0.8091997588078215}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "def metrics(y_true, y_pred, y_proba):\n",
    "    return {\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"Recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"F1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"ROC-AUC\": roc_auc_score(y_true, y_proba),\n",
    "    }\n",
    "\n",
    "base_m = metrics(y_test, y_pred_base, y_proba_base)\n",
    "grid_m = metrics(y_test, y_pred_grid, y_proba_grid)\n",
    "rand_m = metrics(y_test, y_pred_rand, y_proba_rand)\n",
    "\n",
    "print(\"Baseline:\", base_m)\n",
    "print(\"GridSearch:\", grid_m)\n",
    "print(\"RandomSearch:\", rand_m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6eb1e5",
   "metadata": {},
   "source": [
    "## 6 Discuss trade-offs between computational cost and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70efce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a trade-off between computational cost and model performance because more exhaustive hyperparameter searches require significantly more computation while providing only marginal performance improvements. GridSearchCV explores all parameter combinations and may achieve slightly better performance but at high computational cost. RandomizedSearchCV evaluates fewer combinations, reducing cost while still obtaining near-optimal performance. Therefore, in practice, RandomizedSearchCV is often preferred to balance efficiency and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747d68c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
